#!/usr/bin/env python3

import argparse
import os
import subprocess as sp
from os.path import (
    join,
    dirname,
    realpath,
    abspath,
    isfile,
    isdir,
    exists,
    splitext,
    basename,
    getsize,
)
import shutil
import time

import btllib

from .goldrush_edit_utils import get_random_name, watch_process

SEQ_IDS_FILENAME = "seq_ids"
BF_NAME_TEMPLATE = "targeted_k{}.bf"
BFS_DIRNAME = "targeted_bfs"
BFS_BATCH_INPUTPIPE = "targeted_batch"
BFS_BATCH_CONFIRMPIPE = "targeted_confirm"

def get_cli_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("seqs_to_polish")
    parser.add_argument("polishing_seqs")
    parser.add_argument("output_seqs")
    parser.add_argument("-k", action="append", default=[])
    parser.add_argument("-t", "--kmer-threshold", type=int, default=5)
    parser.add_argument("-b", "--bsize", default=1, type=int, help="Batch size.")
    parser.add_argument("-m", "--shared-mem", default="/dev/shm")
    parser.add_argument("-v", "--verbose", action="store_true")

    args = parser.parse_args()
    args.to_polish = abspath(args.to_polish)
    args.reads = abspath(args.reads)
    args.output = abspath(args.output)
    if len(args.k) == 0:
        args.k = [32, 28, 24, 20]

    return args


def autoclean(workspace, prefix):
    process = sp.Popen(["goldrush-edit-autoclean", workspace, prefix])
    watch_process(process)


def build_indexes_and_mappings(polishing_seqs_path, seqs_to_polish_path, k_values):
    # Indexes and mapping filenames
    polishing_seqs_index = f"{basename(polishing_seqs_path)}.index"
    seqs_to_polish_index = f"{basename(seqs_to_polish_path)}.index"
    mappings = f"{basename(seqs_to_polish_path)}.mapping.tsv"
    k_values = " ".join(k_values)

    p = sp.run(
        [
            f""" \
    goldrush-edit-make \
    polishing_seqs={polishing_seqs_path} \
    seqs_to_polish={seqs_to_polish_path} \
    K='{k_values}' \
    {seqs_to_polish_index} \
    {mappings} \
    {polishing_seqs_index} \
  """
        ],
        shell=True,
        text=True,
        capture_output=True,
        check=True,
    )
    btllib.log_info(p.stdout + p.stderr)

    return polishing_seqs_index, seqs_to_polish_index, mappings


def run_bf_builder(
    bfs_dir,
    seqs_to_polish,
    seqs_to_polish_index,
    mappings,
    polishing_seqs,
    polishing_seqs_index,
    kmer_threshold,
    batch_pipepath,
):
    process = sp.Popen(
        [
            "build_targeted_bfs",
            seqs_to_polish,
            seqs_to_polish_index,
            mappings,
            polishing_seqs,
            polishing_seqs_index,
            str(kmer_threshold),
        ],
        cwd=bfs_dir,
    )
    watch_process(process)

    while not exists(batch_pipepath):
        time.sleep(2)
    btllib.log_info("build_targeted_bfs is ready!")

    return process


def get_next_batch_of_contigs(reader, record_ids, output_filepath, batch_size):
    with btllib.SeqWriter(output_filepath) as tmpwriter:
        reader_done = True
        while record := reader.read():
            tmpwriter.write(record.id, record.comment, record.seq)
            record_ids.append(record.id)
            if record.num % batch_size == batch_size - 1:
                reader_done = False
                break
        return reader_done


def make_tmp_dir(workspace, prefix, suffix):
    tmp_dir = join(workspace, f"{prefix}-{suffix}")
    os.mkdir(tmp_dir)
    return tmp_dir


def polish_batch(
    tmp_dir,
    input_pipepath,
    confirm_pipepath,
    to_polish,
    batch_idx,
    record_ids,
    batch_confirmpipe,
    bfs_dir,
    verbose,
):
    with open(input_pipepath, "w") as f:
        print(batch_idx, file=f)
    with open(confirm_pipepath) as f:
        f.read()

    os.mkfifo(join(tmp_dir, batch_confirmpipe))

    with open(join(tmp_dir, RECORD_IDS_FILENAME), "w") as f:
        for id in record_ids:
            print(id, file=f)

    process = sp.Popen(
        ["polish-batch", to_polish, args.K, str(batch_idx), batch_confirmpipe, bfs_dir]
        + (["--verbose"] if verbose else []),
        cwd=tmp_dir,
    )
    watch_process(process)


def wait_on_batch(batch_confirm_pipepath):
    with open(batch_confirm_pipepath) as f:
        f.read()


def write_batch_results(tmp_dir, to_polish, writer):
    seqs_path = join(
        tmp_dir, f"{splitext(to_polish)[0]}.ntedited.prepd.sealer_scaffold.upper.fa"
    )
    if getsize(seqs_path) > 0:
        with btllib.SeqReader(seqs_path, btllib.SeqReaderFlag.LONG_MODE) as tmpreader:
            for record in tmpreader:
                writer.write(record.id, record.comment, record.seq)


def polish_seqs(
    reader,
    writer,
    seqs_to_polish,
    polishing_seqs,
    k_values,
    kmer_threshold,
    batch_size,
    workspace,
    verbose,
):
    prefix = get_random_name()

    # Where to build the Bloom filters
    bfs_dir = make_tmp_dir(workspace, prefix, BFS_DIRNAME)

    # Pipes for communicating with the process building the Bloom filters
    batch_input_pipepath = join(bfs_dir, BFS_BATCH_INPUTPIPE)
    batch_confirm_pipepath = join(bfs_dir, BFS_BATCH_CONFIRMPIPE)

    # Bloom filter names
    bfs = ""
    for k in k_values:
        bf_name = BF_NAME_TEMPLATE.format(k)
        bfs += join(bfs_dir, bf_name) + " "

    # Temporary name for the contig(s) to polish
    contig_tmppath = "contig.tmp.fa"

    # The pipe to communicate with the processes of level two batching
    batch2_confirmpipe = "confirmpipe"

    batch2_paths = []
    handle_sudden_exit(polishing_workspace, prefix)

    polishing_seqs_index, seqs_to_polish_index, mappings = build_indexes_and_mappings(
        reads, to_polish, K
    )

    build_targeted_bfs_process = run_bf_builder(
        bfs_dir,
        to_polish,
        join(cwd, to_polish_index),
        join(cwd, to_polish_read_mapping),
        reads,
        join(cwd, reads_index),
        kmer_threshold,
        input_pipepath,
    )

    btllib.log_info("Polishing batches...")
    reader_done = False
    while not reader_done:
        batch2_paths.clear()

        for batch2_idx in range(batch2_size):
            if reader_done:
                break

            record_ids = []

            tmp_dir = make_tmp_dir(polishing_workspace, prefix, batch2_idx)
            batch2_paths.append(tmp_dir)

            reader_done = get_next_batch_of_contigs(
                reader, record_ids, join(tmp_dir, contig_tmppath), batch1_size
            )

            if len(record_ids) > 0:
                polish_batch(
                    tmp_dir,
                    input_pipepath,
                    confirm_pipepath,
                    contig_tmppath,
                    batch2_idx,
                    record_ids,
                    batch2_confirmpipe,
                    bfs_dir,
                    verbose,
                )
            else:
                shutil.rmtree(tmp_dir, ignore_errors=True)
                batch2_paths.pop()

        for tmp_dir in batch2_paths:
            wait_on_batch(join(tmp_dir, batch2_confirmpipe))
            write_batch_results(tmp_dir, contig_tmppath, writer)
            shutil.rmtree(tmp_dir, ignore_errors=True)

    btllib.log_info("Done polishing batches, ending helper process...")
    with open(input_pipepath, "w") as f:
        print("x", file=f)
    build_targeted_bfs_process.wait()
    shutil.rmtree(bfs_dir, ignore_errors=True)
    btllib.log_info("Polisher done")


if __name__ == "__main__":
    args = get_cli_args()

    # Work in shared memory if possible
    if isdir(args.shared_mem):
        workspace = args.shared_mem
    else:
        workspace = os.getcwd()
        btllib.log_warning(
            f"GoldRush-Edit: {args.shared_mem} not present. Polishing might run slower."
        )

    with btllib.SeqReader(
        args.seqs_to_polish, btllib.SeqReaderFlag.LONG_MODE
    ) as reader, btllib.SeqWriter(args.output) as writer:
        polish_seqs(
            reader,
            writer,
            args.seqs_to_polish,
            args.polishing_seqs,
            args.k,
            args.kmer_threshold,
            args.bsize,
            workspace,
            args.verbose,
        )
